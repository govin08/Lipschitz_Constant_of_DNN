\documentclass[12pt]{report}
\usepackage{amscd,amsthm,amsmath,amstext,amssymb,mathrsfs}
\usepackage{epsfig,longtable,verbatim,graphicx,multirow,array,wasysym,esint,hyperref,kotex,color}
\usepackage[T1]{fontenc}
\usepackage[sort&compress,square,numbers]{natbib}

\numberwithin{figure}{chapter}
\theoremstyle{plain}
\newtheorem{theorem}{\protect\theoremname}[chapter]
\theoremstyle{definition}
\newtheorem{definition}[theorem]{\protect\definitionname}
\theoremstyle{corollary}
\newtheorem{corollary}[theorem]{\protect\corollaryname}
  \theoremstyle{definition}
  \newtheorem{rem}[theorem]{\protect\remarkname}
  \theoremstyle{plain}
  \newtheorem{proposition}[theorem]{\protect\propositionname}
\theoremstyle{definition}
\newtheorem{claim}[theorem]{\protect\examplename}
  \theoremstyle{plain}
  \newtheorem{lemma}[theorem]{\protect\lemmaname}
\newtheorem{assumption}[theorem]{Assumption}

\renewcommand{\labelenumi}{(\roman{enumi})}

\makeatother

  \providecommand{\lemmaname}{Lemma}
  \providecommand{\propositionname}{Proposition}
  \providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}
\providecommand{\examplename}{Claim}
\providecommand{\definitionname}{Definition}
\providecommand{\corollaryname}{Corollary}

\newtheorem{question}{Question}

\newcommand\aint{-\hspace{-0.44cm}\int}
\renewcommand{\theequation}{\thesection.\arabic{equation}}

%% preamble part by ksj
\usepackage[onehalfspacing]{setspace}
\usepackage{booktabs,enumitem}
\newcommand\bs[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand\bx{\ensuremath{\boldsymbol x}}
\newcommand\by{\ensuremath{\boldsymbol y}}
\newcommand\bb{\ensuremath{\boldsymbol b}}
\newcommand\bv{\ensuremath{\boldsymbol v}}
\newcommand\lip{\ensuremath{\text{Lip}}}

\begin{document}
\title{\textbf{Lipschitz Constants of Functions of Neural Networks}}
\author{\\\textbf{Sunjoong Kim}\\}
\setcounter{tocdepth}{1}
\date{}
 \maketitle{}

%\newpage
\tableofcontents
%\listoffigures

\newpage
\begin{abstract}

\end{abstract}
\newpage
\setcounter{page}{1} \setcounter{section}{0}


%%%
\chapter{Introduction}

\setstretch{1.2}

(Lipschitz constant에 대한 간략한 설명 및 neural network과의 관계)

neural network의 Lipschitz constant는 그 neural network의 안정성(robustness)과 관련되어 있다.
Lipschitz constant를 사용해 성능의 개선을 이룬 수많은 사례들이 있는데, 그 중 가장 대표적인 예는 Wasserstein GAN이다.

Lipschitz constant는 크게 global Lipschitz constant와 local Lipschitz constant로 나뉜다.
local Lipschitz constant에 대해 다룬 논문들도 있지만, 이 논문에서는 global한 경우에 대해서만 다루겠다.

아주 간단한 neural network라고 하더라도 optimal Lipschitz constant를 구하는 것이 계산불가능할 수 있음이 알려져있다. (N-P hard)
따라서, optimal Lipschitz constant를 정확히 계산하는 것 보다도 optimal Lipschitz constant의 upper bound찾고, 그 upper bound를 계속해서 개선해나가는 알고리즘들이 알려져있다. 그것들이 3장에서 다루어질 것이다.

2장에서는 유클리드 공간 사이에 정의된 함수에 대한 Lipschitz에 대한 일반적인 사항들을 다룬다.
여기서는 일반적인 함수들에 대해서 다루기도 하지만, 특별히 머신러닝과 관련된 여러 함수들, affine function과 activation function들에 대해서도 다룬다.
3장에서는 우선, 2장에서 서술한 기본적인 원리들을 가지고 MLP의 Lipschitz constant가 어떻게 서술될 수 있는지 다룬다.
이어서 CNN과 RNN과 같은 조금 복잡한 구조에서는 Lipschitz constant가 어떻게 계산될 수 있을지 다룬다.

\setstretch{1.0}


%%%
\chapter{Lipschitz Constants of Functions Between Euclidean Spaces}

%%
\section{Lipschitz Constants}
% Definition : A Liptschitz constant (LC) and The optimal Lipschitz constant(OLC), extension to the metric space functions
Let \(f:\mathbb R^n\to\mathbb R^m\) be a function.
For vectors \(\bx=[x_1\:\:\cdots\:\:x_n]^T\) and \(\by=[y_1\:\:\cdots\:\:y_m]^T\) in Euclidean spaces \(\mathbb R^n\) and \(\mathbb R^m\), respectively, \(||\bx||\) and \(||\by||\) are the usual standard norms of \bx{} and \by{} defined by
\begin{equation}\label{eq:euclidean_norm}
||x||=\sqrt{{x_1}^2+\cdots+{x_n}^2},\quad ||y||=\sqrt{{y_1}^2+\cdots+{y_m}^2}.
\end{equation}

\begin{definition}\label{LC}
A nonnegative real number \(C\) is called a Liptschitz constant of \(f\) if
\begin{equation}\label{eq:LC1}
||f(\bx)-f(\by)||\le C||\bx-\by||\qquad(\bx,\by\in\mathbb R^n)
\end{equation}
The infimum of \(C\) which satisfies \eqref{eq:LC1} is called the optimal Lipschitz constant of \(f\) and is denoted by \(\lip(f)\).
If \(\lip(f)\) exists as a nonnegative real number, \(f\) is called a Lipschitz continuous function.
\end{definition}

% The equality of inf-def and sup-def
This definition can also be extended to functions from a metric space to another metric space if we replace the norms by the distances.
It is useful to know that there is an equivalent definition ;
\begin{equation}\label{eq:LC2}
\lip(f)=\sup_{\bx\neq\by}\frac{||f(\bx)-f(\by)||}{||\bx-\by||}
\end{equation}
To prove the equality, let
\begin{equation}\label{eq:equivalent_def}
\begin{aligned}
L_1&=\inf\{C\ge0:||f(\bx)-f(\by)||\le C||\bx-\by||,\quad\bx,\by\in\mathbb R^n\}\\
L_2&=\sup\left\{\frac{||f(\bx)-f(\by)||}{||\bx-\by||}:\bx,\by\in\mathbb R^n,\bx\neq\by\right\}.
\end{aligned}
\end{equation}
Let \(\mathcal L_1\) and \(\mathcal L_2\) be two the sets in the definition of \(L_1\) and \(L_2\), respectively.
Then, \(\mathcal L_2\subset\mathcal L_1\) since
\[\mathcal L_2=\{C\ge0:||f(\bx)-f(\by)||=C||\bx-\by||,\quad\bx,\by\in\mathbb R^n,\bx\neq\by\}.\]
Pick \(C\in\mathcal L_2\).
Then, \(C\in\mathcal L_1\) and it follows that \(L_1=\inf\mathcal L_1\le C\).
Since \(C\in\mathcal L_2\) was arbitrary, we can take supremum for all \(C\in\mathcal L_2\) to conclude that \(L_1\le L_2\).
Pick \(C\in\mathcal L_1\).
Then, \(||f(\bx)-f(\by)||\le C||\bx-\by||\) for all \(\bx\) and \(\by\) in \(\mathbb R^n\).
Assuming \(\bx\neq \by\), we have
\[
\frac{||f(\bx)-f(\by)||}{||\bx-\by||}\le C.
\]
Taking supremum for all \bx{} and \by{} (\(\bx\neq\by\)), we have \(L_2\le C.\) and taking infimum for all \(C\le\mathcal L_1\) yields \(L_2\le L_1\).

% inf = min
The optimal Lipschitz constant \(\lip(f)\) is not only the infimum, but also the minimum of \(L\) satisfying \eqref{eq:LC1}.
To show this, it suffices to show that \(\lip(f)\in\mathcal L_1\), or that \(C=\lip(f)\) satisfies the condition \eqref{eq:LC1}.
Let \(\bx,\:\by\in\mathbb R^n\).
If \(\bx=\by\), then \eqref{eq:LC1} holds trivially.
Assuming \(\bx\neq\by\), we have
\[
\frac{||f(\bx)-f(\by)||}{||\bx-\by||}\le\lip(f)
\]
because of the second definition of \(\lip(f)\).
Multiplying both sides by \(||\bx-\by||\), we can conclude that \(C=\lip(f)\) satisfies the condition \eqref{eq:LC1}.
% sup = max도 증명하면 좋겠는데, 어떻게 해야 할 지 모르겠다.
% f가 linear map인 경우에는 conrad의 방법을 사용하면 된다.
% conrad의 방법을 여기에 쓰자니, 가능은 할 것 같은데 말이 너무 길어지는 것 같다.

% lip=0 iff constant function
Note also that \(\lip(f)=0\) if and only if \(f\) is a constant function.
Suppose that \(f\) is a constant function.
Then, the condition \eqref{eq:LC1} holds vacuously and \(\mathcal L_1=[0,\infty)\) ; \(\lip(f)=0\).
Suppose, on the contrary, that \(\lip(f)=0\).
Pick \(\bx,\by\in\mathbb R^n\).
Since \(0\in\mathcal L_1\), we have \(||f(\bx)-f(\by)||=0\).
Thus, \(f(\bx)=f(\by)\) and \(f\) is a constant function.
% Finding OLC is NP-hard [1]

%%
\section{Real-valued Functions of a Real Variable}
% Average rate of change
% differentiable (almost) everywhere → mean value theorem
% sigmoid, hyperbolic tangent, ReLU

For univariate real function \(f:\mathbb R\to\mathbb R\), the above condition \eqref{eq:LC1} is equivalent to saying that the average rate of change is upper bounded by \(C\);
\begin{equation}\label{eq:LC_average_rate_of_change}
\left|\frac{f(x)-f(y)}{x-y}\right|\le C.
\end{equation}
If \(f\) is diffrentiable everywhere in the domain, the mean value theorem guarantees that this is equivalent to 
\begin{equation}\label{eq:LC_derivative}
|f'(x)|\le C
\end{equation}
for every \(x\in\mathbb R\).
Moreover, \(\lip(f)\) is the least nonnegative number \(C\) satisfying the condition \eqref{eq:LC_derivative} for all \(x\).

In this sense, we can easily evaluate the optimal Lipschitz constant of the sigmoid function
\begin{equation}\label{eq:sigmoid}
\sigma(x)=\frac1{1+e^{-x}}
\end{equation}
and the hyperbolic tangent function
\begin{equation}\label{eq:tanh}
\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}.
\end{equation}
Since \(0<\sigma(x)<1\) and \(\sigma'(x)=\sigma(x)\left(1-\sigma(x)\right)\), we have \(\sigma'(x)<\frac14\) so that
\begin{equation}\label{eq:sigmoid_OLC}
\lip(\sigma)=\frac14.
\end{equation}
Since \(\tanh(x)=2\sigma(2x)-1\), we have
\[\tanh'(x)=4\sigma'(2x)<1\]
and
\begin{equation}\label{eq:tanh_OLC}
\lip(\tanh)=1.
\end{equation}

The \text{ReLU} function
\begin{equation}\label{eq:ReLU}
\text{ReLU}(x)=\max\{0,x\}
\end{equation}
 is not differentiable everywhere, but we can use \eqref{eq:LC_average_rate_of_change} to conclude
\begin{equation}\label{eq:ReLU_OLC}
\lip(\text{ReLU})=1.
\end{equation}
%Although not being differentiable, the case of rectified linear unit \(\text{ReLu}(x)=\max\{0,x\}\) can also be easily evaluated to be \(1\).
Here is a list of the optimal Lipschitz constants of univariate activation functions, frequently used in machine learning and deep learning.
 
\renewcommand\arraystretch{1.5}
\begin{table}[ht]
\centering
\caption{The optimal Lipschitz constants of univariate activation functions.
\(\alpha=0.1\) for Leaky ReLU and \(\alpha=1\) for ELU.}
\begin{tabular}[t]{llc}
\toprule
Activation Functions	&Formula									& $\lip(f)$\\
\midrule
Sigmoid				&$\sigma(x)=\frac1{1+e^{-x}}$				&$\frac14$\\
Hyperbolic tangent	&$\tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$	&1\\
Rectified Linear Unit	&$\text{ReLU}(x)=\max\{0,x\}$			&1\\
Leaky ReLU			&$\text{LReLU}(x)=\max\{\alpha x,x\}$	&1\\
Exponential Linear Unit&$\text{ELU}(x)=
\begin{cases}x&(x\ge0)\\\alpha(e^x-1)&(x<0)\end{cases}$	&1\\
Gaussian			&$f(x)=e^{-x^2}$								&$\sqrt{\frac2e}$\\
\bottomrule
\end{tabular}
\end{table}

In the definition of \(\lip(f)\) in \eqref{eq:LC2}, we can't replace the supremum by the maximum.
Consider a function ; \(f(x)=\frac1{\ln(1+e^x)}\).
This function is called the softplus activation function and is an antiderivative of the sigmoid function.
Both of the function itself and the derivative of it are monotonically increasing, with an assymptote \(y=x\).
Thus, \(\lip(f)=1\), but no \(x\) and \(y\) satisfy the equality \(\frac{f(x)-f(y)}{x-y}=1\).

%%
\section{Linear Functions}
% Operator norms of matrices matches the notion of OLC

%
\subsection{Operator Norms \(||W||\)}

Consider a linear function \(f(\bx)=W\bx\) for some \(m\times n\) matrix \(W\).
Because of the linearity, the condition \eqref{eq:LC1} reduces to
\begin{equation}\label{eq:linear_LC}
||W\bx||\le c||x||.\qquad(\bx\in\mathbb R^n)
\end{equation}

The smallest \(C\) which satisfies \eqref{eq:linear_LC} is called \emph{the operator norm of \(W\)} and denoted by \(||W||\).
Thus, the optimal Lipstchitz constant of \(f\) equals the operator norm of \(W\) ; \(||W||=\lip(f)\).
The definitions of the forms \eqref{eq:equivalent_def} are as follows;
\begin{equation}\label{eq:linear_equivalent_def}\begin{aligned}
||W||
&=\inf\left\{c\ge0\::\: ||W\bx||\le c||\bx||\right\}\\
&=\sup\left\{\frac{||W\bx||}{||\bx||}\;:\;\bx\in\mathbb R^n,\:\bx\neq\bs0\right\}
\end{aligned}\end{equation}
If we modify the second definition of \eqref{eq:linear_equivalent_def} to
\begin{equation}\label{eq:linear_equivalent_def_2}
||W||=\sup\left\{||W\bx||:||\bx||=1\right\},
\end{equation}
we can easily verify that the supremum is actually the maximum in this case.
Because the set \(\{\bx:||\bx||=1\}\) is a compact subset of \(\mathbb R^m\) and the map \(\bx\mapsto||W\bx||\) is continuous, the set in \eqref{eq:linear_equivalent_def_2} has its maximum.

The operator norm is indeed a norm of a vector space ;
\begin{proposition}\label{prop:operator_norm_1}
The operator \(||\cdot||:W\mapsto||W||\) satisfies the following three properties.
Thus, the set \(\mathcal M_{m,n}(\mathbb R)\) of all \(m\) by \(n\) real matrices is a normed vector space with respect to this norm ;
\begin{enumerate}[label=(\alph*)]
\item
\(||W||\ge0\) for all \(W\in\mathcal M_{m,n}(\mathbb R)\) ; \(||W||=0\) if and only if \(W=0\).
\item
\(||kW||=|k|\cdot||W||\) for all \(W\in\mathcal M_{m,n}(\mathbb R)\) and \(k\in\mathbb R\).
\item
\(||W+V||\le||W||+||V||\) for all \(W,V\in\mathcal M_{m,n}(\mathbb R)\).
\end{enumerate}
\end{proposition}
For (a), that \(||W||\ge0\) is obvious.
If \(||W||=0\), then \(W\bx=0\) for all \(\bx\in\mathbb R^n\).
Taking \(\bx=e_i(1\le i\le n)\), the \(i\)-th columns of \(W\) are zero vectors.
Therefore, \(W=0\).
Suppose, on the other hand, that \(W=0\). Then \(W\bx=0\) for all \(x\in\mathbb R^n\).
Thus, \(c=0\) is the minimal value satisfying \eqref{eq:linear_LC}.
To prove (b), we have
\begin{align*}
||kW||
&=\inf\left\{c:||kWx||\le c||x||\right\}\\
&=\inf\left\{c:|k|\cdot||Wx||\le c||x||\right\}\\
&=\inf\left\{c:||Wx||\le\frac{c}{|k|}||x||\right\}\\
&=\inf\left\{|k|b:||Wx||\le b||x||\right\}\\
&=|k|\inf\left\{b:||Wx||\le b||x||\right\}\\
&=|k|\cdot||W||.
\end{align*}
Now, consider (c).
For any \(\bx\in\mathbb R^n\), we have
\begin{align*}
||(W+V)\bx||
&=||W\bx+V\bx||\le||W\bx||+||V\bx||\\
&\le||W||\:||\bx||+||V||\:||\bx||=(||W||+||V||)||\bx||
\end{align*}
By the minimality of \(||W+V||\), we have \(||W+V||\le||W||+||V||\).

An analogue of (c) also holds for multiplication;
if \(W\in\mathcal M_{m,n}(\mathbb R)\) and \(V\in\mathcal M_{n,k}(\mathbb R)\), then
\begin{equation}\label{eq:multiplicative_inequality}
||WV||\le||W||\:||V||.
\end{equation}
This is because of the equation
\[||(WV)\bx||=||W(V\bx)||\le||W||\:||V\bx||\le||W|\:||V||\:||\bx||\]
and the minimality of \(||WV||\).

%
\subsection{Evaluation of \(||W||\) for square matrices}

First, consider the case when \(W\) is a square matrix so that \(f:\mathbb R^n\to\mathbb R^n\).
Let \(\lambda_1\), \(\cdots\), \(\lambda_n\) be (possibly repeated) eigenvalues of \(W\).
 and let \(\tilde\lambda=\max\{|\lambda_i|\::\:i=1,\cdots,n\}\).
Then, the following inequality holds in general ;
\begin{equation}\label{eq:square_matrix_equation_1}
\tilde\lambda\le||W||.
\end{equation}
For a proof, pick an eigenvalue \(\lambda_i\) and the corresponding eigenvector \(\bx_i\), which is nonzero.
Since \(W\bx_i=\lambda_i\bx_i\),
Then, we have \(||W\bx_i||=|\lambda_i|\:||\bx_i||\) and
\[|\lambda_i|=\frac{||W\bx_i||}{||\bx_i||}\le||W||.\]
Taking the maximum over \(i\), we have the desired inequality.
The above inequality \eqref{eq:square_matrix_equation_1} becomes equality when \(W\) is symmetric.

Suppose that \(W\) is real symmetric.
Then, \(W\) is orthogonally diagonalizable in the sense that, there exists an orthogonal matrix
\[V=[\bv_1\quad \cdots \quad \bv_n],\]
%which is orthogonal (and thus \(\{\bv_1,\cdots,\bv_n\}\) is orthonormal.)
such that
\[W=V\Lambda V^T,\]
where \(\Lambda=\text{diag}\{\lambda_1,\cdots,\lambda_n\}\) and \(\lambda_i\) is an eigenvalue of \(W\) with the corresponding eigenvector \(\bv_i\).
Note that \(\{\bv_1,\cdots,\bv_n\}\) forms an orthonormal basis for \(\mathbb R^n\).
For any \(\bx\in\mathbb R^n\), there exist real numbers \(c_1\), \(\cdots\), \(c_n\) such that \(\bx=c_1\bv_1+\cdots+c_n\bv_n\).
Since
\begin{align*}
W\bx
&=c_1W\bv_1+\cdots+c_nW\bv_n\\
&=c_1\lambda_1\bv_1+\cdots+c_n\lambda_n\bv_n,
\end{align*}
we have
\begin{align*}
||W\bx||^2
&=||c_1W\bv_1+\cdots+c_nW\bv_n||^2\\
&=|c_1\lambda_1|^2||\bv_1||^2+\cdots+|c_n\lambda_n|^2||\bv_n||^2\\
&={c_1}^2{\lambda_1}^2+\cdots+{c_n}^2{\lambda_n}^2.
\end{align*}
Thus,
\[\frac{||W\bx||^2}{||\bx||^2}=\frac{{c_1}^2{\lambda_1}^2+\cdots+{c_n}^2{\lambda_n}^2}{{c_1}^2+\cdots+{c_n}^2}
\le{\tilde\lambda}^2,\]
for which \(||W\bx||/||\bx||\le\tilde\lambda\).
Therefore, we have the reverse inequality \(||W||\le\tilde\lambda\).

As examples consider the following matrices \(W_1\), \(W_2\), \(W_3\), \(W_4\), \(W_5\) defined by
\begin{gather*}
W_1=\begin{bmatrix}
1&0\\0&1
\end{bmatrix},\quad
W_2=\begin{bmatrix}
2&0\\0&3
\end{bmatrix},\quad
W_3=\begin{bmatrix}
4&2\\2&7
\end{bmatrix},\\
W_4=\begin{bmatrix}
1&3\\2&0
\end{bmatrix},\quad
W_5=\begin{bmatrix}
0&1\\0&0
\end{bmatrix}.\quad
\end{gather*}
\(W_1\) is the identity matrix and thus \(||W||=1\).
Indeed, the characteristic polyinomial of \(W_1\) is \((\lambda-1)^2=0\), \(\lambda_1=\lambda_2=1\) and thus the maximum absolute value of them is \(1\).
The matrix \(W_2\) stretches a given vector \(\bx\) twice in the x-axis direction and three times in the y-axis direction.
Thus, \(||W||\) should be \(3\).
Indeed, \(|W_2-\lambda I|=(\lambda-2)(\lambda-3)\) and \(\tilde\lambda=3\).
\(W_3\) is the last example that is symmetric ; \(|W_3-\lambda I|=(\lambda-3)(\lambda-8)\) and \(\tilde\lambda=8\).

\(W_4\) is not symmetric and we can't evaluate \(||W_4||\) for now.
Still, we have, \(\lambda_1=-2\), \(\lambda_2=3\) and \(||W||\le3\).
The reverse inequality is not valid since the eigenvectors \(x_1=[1\:\:-1]^T\) and \(x_2=[3\:\:2]^T\) are not perpendicular.
\(W_5\) is not symmetric either, but we can postulate that \(||W_5||=1\) since
\[
W_5\bx=
\begin{bmatrix}
0&1\\0&0
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
x_2\\0
\end{bmatrix}
\]
and
\[
\frac{||W_5\bx||}{||\bx||}=\frac{|x_2|}{\sqrt{{x_1}^2+{x_2}^2}}\le 1.
\]
Since \(W_5\) has the only eigenvector \(0\), the inequality \eqref{eq:square_matrix_equation_1} still holds.

Here is a short description of what we've done in this subsection.
\begin{lemma}\label{lemm:evaluation_of_operator_norm_for_square_matrix}
Let \(W\in\mathcal M_n(\mathbb R)\) be a real symmetric matrix.
Then, the operator norm \(||W||\) equals the maximum absolute value of eigenvalues of \(W\).
\end{lemma}

%
\subsection{Evaluation of \(||W||\) for rectangular matrices}
Almost every matrix that we encounter in machine learning are not square matrices, nor are they symmetric matrices.
But, we can always calculate the optimal Lipschitz constant or the operator norm of any rectangular matrices.
Aside from the properties listed in the Proposition \ref{prop:operator_norm_1}, there is a useful criteria that help find the optimal Lipschitz constant of any rectangular matrix.

\begin{theorem}\label{theo:evaluation_of_operator_norm_for_rectangular_matrix}
Let \(W\in\mathcal M_{m,n}(\mathbb R)\) be an \(m\) by \(n\) real matrix.
Then
\begin{equation}\label{eq:evaluation_of_operator_norm_for_rectangular_matrix}
||W||=\sqrt{||W^TW||}.
\end{equation}
\end{theorem}

Note that the norm on the left hand side is the operator norm on \(\mathcal M_{m,n}(\mathbb R)\) and the norm on the right hands side is the operator norm on \(\mathcal M_n(\mathbb R)\).
Note also that the matrix \(W^TW\) is symmetric in that \((W^TW)^T=W^T(W^T)^T=W^TW\) and we are always able to calculate the norm of \(||W^TW||\) by making use of the lemma \ref{lemm:evaluation_of_operator_norm_for_square_matrix}.

Before the proof of \eqref{eq:evaluation_of_operator_norm_for_rectangular_matrix}, we illustrate elementary properties of the operator norm of matrices.
The euclidean norm \(||\bx||\) of a vector \(\bx\in\mathbb R^n\) in \eqref{eq:euclidean_norm} can also be defined by means of the inner product on \(\mathbb R^n\);
\begin{equation}\label{eq:euclidean norm_2}
||\bx||=\sqrt{\langle \bx,\bx\rangle}.
\end{equation}
The inner product \(\langle \bx,\by\rangle=\bx^T\by\) has the following properties and we omit the proofs of them.
\begin{gather*}
\langle \bx,W\by\rangle=\langle W^T\bx,\by\rangle,\\
\langle k\bx,\by\rangle=k\langle \bx,\by\rangle,\\
|\langle\bx,\by\rangle|\le||\bx||\:||\by||.
\end{gather*}

Now we are ready for the proof of \eqref{eq:evaluation_of_operator_norm_for_rectangular_matrix}.
If \(W=0\), then the equation \eqref{eq:evaluation_of_operator_norm_for_rectangular_matrix} holds trivially.
Suppose that \(W\neq0\).
Let \(\bx\in\mathbb R^n\).
Then
\begin{align*}
||W\bx||^2
&=\langle W\bx,W\bx\rangle\\
&=\langle W^TW\bx,\bx\rangle\\
&\le||W^TW\bx||\:||\bx||\\
&\le||W^TW||\:||x||^2,
\end{align*}
and thus
\[||W\bx||\le\sqrt{||W^TW||}\:||x||.\]
By the minimality of \(||W||\), we have
\[||W||\le\sqrt{||W^TW||}\]
Squaring both sides and making use of \eqref{eq:multiplicative_inequality} yield
\begin{equation}\label{eq:half_inequality_2}
||W||^2=||W^TW||\le||W^T||\:||W||.
\end{equation}
Since \(||W||\neq0\), we have
\[||W||\le||W^T||.\]
Substituting \(W\) by its transpose, we get the reverse inequality.
Therefore,
\begin{equation}\label{eq:norm_and_transpose}
||W||=||W^T||.
\end{equation}
By \eqref{eq:norm_and_transpose}, the equation \eqref{eq:half_inequality_2} reduces to
\[||W||^2=||W^TW||,\]
and this proves \eqref{eq:evaluation_of_operator_norm_for_rectangular_matrix}.

For example, we can evaluate the operator norm of \(W_5\) in the previous subsection.
Since
\[{W_5}^TW_5=\begin{bmatrix}0&0\\0&1\end{bmatrix},\]
\({W_5}^TW_5\) has eigenvalues \(0\) and \(1\) ; \(||W_5||=\sqrt1=1\) as expected.
As a rectangular matrix, we may think of
\[W_6=\begin{bmatrix}1&0\\2&1\\0&1\end{bmatrix}.\]
Since
\[{W_6}^TW_6=\begin{bmatrix}5&2\\2&2\end{bmatrix},\]
\(\lambda=1,6\) and \(||W_6||=\sqrt6\).

%%
\section{Affine Functions}
An affine function 
\begin{equation}\label{eq:affine}
f(\bx)=W\bx+\bb
\end{equation}
has the same optimal Liptschitz constant as that of its linear part ; 
\begin{equation}\label{eq:affine_OLC}
\lip(f)=||W||
\end{equation}
This is because the condition \eqref{eq:LC1} 
\[
||f(\bx)-f(\by)||\le C||\bx-\by||
\]
applied to \eqref{eq:affine} becomes
\[
||W\bx-W\by||\le C||\bx-\by||.
\]

%%
\section{Elementwise Application of Nonlinear Functions}
Let \(f:\mathbb R\to\mathbb R\) be any function and let \(F:\mathbb R^m\to\mathbb R^m\) be applying \(f\) to each element of the input \(\bx = [x_1\:\cdots\:x_m]^T\) :
\begin{equation}\label{eq:elementwise}
F(\bx)=F\left(\begin{bmatrix}x_1\\\vdots\\x_m\end{bmatrix}\right)=
\begin{bmatrix}f(x_1)\\\vdots\\f(x_m)\end{bmatrix}.
\end{equation}
Let \(f\) be a Liptschitz continuous function with its optimal constant \(L\).
Then,
\begin{align*}
||F(\bx)-F(\by)||^2
&=\left|\left|\begin{bmatrix}f(x_1)-f(y_1)\\\vdots\\f(x_m)-f(y_m)\end{bmatrix}\right|\right|^2
=\sum_{i=1}^m|f(x_i)-f(y_i)|^2\\
&\le\sum_{i=1}^mL^2|x_i-y_i|^2=L^2||\bx-\by||^2,
\end{align*}
for which
\[||F(\bx)-F(\by)||\le L||\bx-\by||.\]
Thus, \(L\) is a Liptschitz constant of \(F\) and the optimal Lipschitz constant of \(F\) is upper bounded by \(L\) ;
\begin{equation}\label{eq:elementwise_OLC}
\lip(F)\le\lip(f).
\end{equation}

%%
\section{Composites of Functions}
Let \(f:\mathbb R^n\to\mathbb R^m\) and \(g:\mathbb R^m\to\mathbb R^k\) be Lipstchitz continuous functions.
Then, the composite function \(g\circ f\) is also Lipstchitz continuous and the optimal constant is bounded by the product of those of \(f\) and \(g\) ; 
\begin{equation}\label{eq:composite_1}
\lip(g\circ f)\le\lip(f)\lip(g)
\end{equation}

Let \(L\) and \(M\) be the optimal Lipschitz constants for \(f\) and \(g\) respectively.
Then, for all \(\bx,\by\in\mathbb R^n\),
\begin{align*}
||(g\circ f)(\bx)-(g\circ f)(\by)||
&=||g(f(\bx))-g(f(\by))||\\
&\le M||f(\bx)-f(\by)||\\
&\le LM||\bx-\by||.
\end{align*}
By the minimality of \(\lip(g\circ f)\), we have \eqref{eq:composite_1}.

Note that \eqref{eq:composite_1} is a generalization of \eqref{eq:multiplicative_inequality}.
The inequality can be strict.
As an example, \(||W_2||\:||W_4||=3\times\sqrt{7+\sqrt{13}}>\sqrt{1+\sqrt{37}}=||W_2W_4||\).
Note also that \eqref{eq:composite_1} can be generalized further, to the case when the composition involoves several functions.
That is, if \(f_i:\mathbb R^{n_{i-1}}\to\mathbb R^{n_i}\) are  functions between euclidean spaces, with its optimal Lipschitz constant \(L_i\), for \(i=1,2,\cdots,p\), the composite
\[f_p\circ\cdots\circ f_1:\mathbb R^{n_1}\to\mathbb R^{n_{p+1}}\]
is Lipschitz constinuous and
\begin{equation}\label{eq:composite_2}
\lip(f_p\circ\cdots\circ f_1)\le\prod_{i=1}^kL_i.
\end{equation}

%%%
\chapter{Lipschitz Constants of Neural Networks}

Now we turn to the problem of the optimal Lischitz constant of various architectures which appear in machine learning and deep learning.
In this chapter, we use the term "Lipschitz constant of a function" to stand for the optimal Lipschitz constant of a function.

If \(f:\mathbb R^n\to\mathbb R^m\) is Lipschitz continuous almost everywhere, it is possible to express the Lipschitz constant of \(f\) in its exact form, by Rademacher Theorem.
But, it is not always easy or feasible to find the exact value of Lipschitz constant whenever the function \(f\) is given.
In fact, although the function is of simple form such as 2-layered MLP, the problem of evaluating the Lipschitz function is NP-hard.
Instead of struggling to find analytic solutions, we present a systematic algorithm for each architectuare of neural network.

%%
\section{Rademacher Theorem}

%%
\section{Multi-Layer Perceptron}

%%
\section{Dropout and Normalization}

%%
\section{CNN}

%%
\section{RNN}

%%%
\chapter{Conclusion}



\newpage



\newpage

\addcontentsline{toc}{chapter}{Bibliography}
\begin{thebibliography}{AA}
\bibitem {ACC} C. Adams, M. Chu, T. Crawford, S. Jensen, K. Siegel and L. Zhang,
    {\em Stick index of knots and links in the cubic lattice},
    J. Knot Theory Ramif. \textbf{21} (2012) 1250041.



\end{thebibliography}



%\addcontentsline{toc}{chapter}{Bibliography}
%
%\bibliographystyle{abbrv}
%\bibliography{reference.bib}
%
%



\end{document}

